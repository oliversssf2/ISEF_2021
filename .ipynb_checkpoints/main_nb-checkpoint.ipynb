{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dplm_base\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dplm_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are linkage, length, mass\n",
      "alpha_o1o_1:0\n",
      "alpha_o2o4:0\n",
      "alpha_o2o_2:0\n",
      "alpha_o3o_3:0\n",
      "alpha_o4o_4:0\n",
      "alpha_o_1o_2:0\n",
      "l_o1o2:0.254\n",
      "l_o1o3:0.10699999999999998\n",
      "l_o1o_1:0.762\n",
      "l_o2o4:0.615\n",
      "l_o2o_2:0.762\n",
      "l_o3o4:0.254\n",
      "l_o3o_3:0.648\n",
      "l_o4o_4:0.648\n",
      "l_o_1o_2:0.254\n",
      "l_o_3o_4:0.254\n",
      "m_o1o2:0.29870661\n",
      "m_o1o_1:0.84734661\n",
      "m_o2o4:0.67904403\n",
      "m_o2o_2:0.84734661\n",
      "m_o3o_3:0.72422661\n",
      "m_o4o_4:0.72422661\n",
      "m_o_1o_2:0.29870661\n",
      "r_o1o_1:0.381\n",
      "r_o2o4:0.3075\n",
      "r_o2o_2:0.381\n",
      "r_o3o_3:0.324\n",
      "r_o4o_4:0.324\n",
      "r_o_1o_2:0.127\n",
      "The number of slots is set to 10\n"
     ]
    }
   ],
   "source": [
    "dplm_instance = dplm_base.dplm('para1.csv')\n",
    "dplm_instance.show_dplm_config()\n",
    "dplm_instance.set_dplm_slot_num(10)\n",
    "dplm_instance.set_dplm_spring_num(3)\n",
    "# dplm_instance.set_springs_positions([0.16,0.16,0.16])\n",
    "dplm_instance.set_dplm_spring_constants([300,300,300])\n",
    "dplm_instance.set_dplm_spring_lengths([0.16,0.16,0.16])\n",
    "dplm_instance.set_dplm_allowed_angle_range(-20, 60, 1)\n",
    "# dplm_instance.set_slot([-6, 18, 0])\n",
    "# moment_weight, moment_spring_list, moment_total = dplm_instance.calculate_current_moment()\n",
    "# lower_limit, upper_limit, step_size, total_angle_num = dplm_instance.get_allowed_angle_range().values()\n",
    "# 26 32 38\n",
    "#13 37 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = dplm_instance.calculate_current_moment()\n",
    "%matplotlib inline\n",
    "plt.cla()    \n",
    "plt.plot(range(lower_limit, upper_limit+1), a, label = 'moment_weight', ls = '--', lw = 3, color = 'mediumaquamarine')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for i in range(len(moment_spring_list)):\n",
    "    plt.plot(range(lower_limit,upper_limit+1), b[i], label = 'moment_spring_{}'.format(i+1), ls = '--', lw = 3, color = 'cornflowerblue')\n",
    "\n",
    "plt.plot(range(lower_limit, upper_limit+1), c, label = 'moment_total', ls = '--', lw = 3, color = 'mediumslateblue')\n",
    "plt.axhline(y = 0, ls = '-', lw = 3, color = 'darkgrey')\n",
    "\n",
    "plt.axis(ymin=-20, ymax=50)\n",
    "plt.legend()\n",
    "plt.xlabel('angle [degree]')\n",
    "plt.ylabel('moment [Nm]')\n",
    "ax.xaxis.set_major_formatter('{x}Â°')\n",
    "\n",
    "\n",
    "plt.text(-10,-10, r'$RMSE={:.2f}$'.format(dplm_instance.current_rmse()))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(dplm_instance.current_rmse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "cwd = os.getcwd()\n",
    "angle_range = {\n",
    "    'lower_limit' : -20,\n",
    "    'upper_limit' : 60,\n",
    "    'step_size' : 1\n",
    "}\n",
    "\n",
    "env = gym.make('gym_dplm:dplm-v0', \n",
    "                dplm_config_file = cwd+\"/para1.csv\",\n",
    "                spring_num = 3,\n",
    "                slot_num = 20,\n",
    "                spring_constants = [300,300,300],\n",
    "                spring_init_lengths = [0.16,0.16,0.16],\n",
    "                **angle_range)\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A callback for displaying a progress bar while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "    def __init__(self, pbar):\n",
    "        super(ProgressBarCallback, self).__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "            \n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are linkage, length, mass\n",
      "The number of slots is set to 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fongsu/.pyenv/versions/r_learning_gym/lib/python3.8/site-packages/stable_baselines3/common/cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5302c668290d42adb2f41c123828d425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | 12       |\n",
      "| time/              |          |\n",
      "|    fps             | 2713     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "\n",
    "cwd = os.getcwd()\n",
    "angle_range = {\n",
    "    'lower_limit' : -20,\n",
    "    'upper_limit' : 60,\n",
    "    'step_size' : 1\n",
    "}\n",
    "\n",
    "env = gym.make('gym_dplm:dplm-v0', \n",
    "                dplm_config_file = cwd+\"/para1.csv\",\n",
    "                spring_num = 3,\n",
    "                slot_num = 20,\n",
    "                spring_constants = [300,300,300],\n",
    "                spring_init_lengths = [0.16,0.16,0.16],\n",
    "                rmse_limit = 3,\n",
    "                **angle_range)\n",
    "\n",
    "# env = make_vec_env(lambda: env, n_envs=40)\n",
    "\n",
    "env = make_vec_env(lambda: env, n_envs=10)\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "model.learn(10000)\n",
    "\n",
    "\n",
    "# model = A2C('MlpPolicy', env, verbose=1).learn(50000)\n",
    "# model.save('dplm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: NO.1\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [19 37 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.2\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [24  1 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.3\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [21 22 31] reward= 0.2714244322295605 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [21 22 32] reward= 0.24974995263327573 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [21 22 33] reward= 0.17505276296766628 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [21 22 34] reward= 0.12543690369722038 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [21 22 35] reward= 0.09583023821137233 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [21 22 36] reward= 0.07699310117633823 done= False\n",
      "Step 7\n",
      "Action:  5\n",
      "obs= [21 22 37] reward= 0.06415254623950921 done= False\n",
      "Step 8\n",
      "Action:  5\n",
      "obs= [21 22 38] reward= 0.05490161220056611 done= False\n",
      "Step 9\n",
      "Action:  5\n",
      "obs= [21 22 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.4\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [19 19  7] reward= 0.020136340607979263 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [19 19  8] reward= 0.021196089004483536 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [19 19  9] reward= 0.022354056458370394 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [19 19 10] reward= 0.023618006165949907 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [19 19 11] reward= 0.02499331085428743 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [19 19 12] reward= 0.0264804892191301 done= False\n",
      "Step 7\n",
      "Action:  5\n",
      "obs= [19 19 13] reward= 0.028071631838981214 done= False\n",
      "Step 8\n",
      "Action:  5\n",
      "obs= [19 19 14] reward= 0.029746092889621184 done= False\n",
      "Step 9\n",
      "Action:  5\n",
      "obs= [19 19 15] reward= 0.031467187233933595 done= False\n",
      "Step 10\n",
      "Action:  5\n",
      "obs= [19 19 16] reward= 0.03318367850528616 done= False\n",
      "Step 11\n",
      "Action:  5\n",
      "obs= [19 19 17] reward= 0.0348405508203258 done= False\n",
      "Step 12\n",
      "Action:  5\n",
      "obs= [19 19 18] reward= 0.03639916057213613 done= False\n",
      "Step 13\n",
      "Action:  5\n",
      "obs= [19 19 19] reward= 0.03785799481805438 done= False\n",
      "Step 14\n",
      "Action:  5\n",
      "obs= [19 19 20] reward= 0.039262365047895015 done= False\n",
      "Step 15\n",
      "Action:  5\n",
      "obs= [19 19 21] reward= 0.04070193142340982 done= False\n",
      "Step 16\n",
      "Action:  5\n",
      "obs= [19 19 22] reward= 0.042309313035773136 done= False\n",
      "Step 17\n",
      "Action:  5\n",
      "obs= [19 19 23] reward= 0.04434597533275755 done= False\n",
      "Step 18\n",
      "Action:  5\n",
      "obs= [19 19 24] reward= 0.04712513140518828 done= False\n",
      "Step 19\n",
      "Action:  5\n",
      "obs= [19 19 25] reward= 0.05095200095397368 done= False\n",
      "Step 20\n",
      "Action:  5\n",
      "obs= [19 19 26] reward= 0.056311535705607506 done= False\n",
      "Step 21\n",
      "Action:  5\n",
      "obs= [19 19 27] reward= 0.06408122256137232 done= False\n",
      "Step 22\n",
      "Action:  5\n",
      "obs= [19 19 28] reward= 0.0755756428994924 done= False\n",
      "Step 23\n",
      "Action:  5\n",
      "obs= [19 19 29] reward= 0.09294993304631072 done= False\n",
      "Step 24\n",
      "Action:  5\n",
      "obs= [19 19 30] reward= 0.12135672176086056 done= False\n",
      "Step 25\n",
      "Action:  5\n",
      "obs= [19 19 31] reward= 0.174453690250494 done= False\n",
      "Step 26\n",
      "Action:  5\n",
      "obs= [19 19 32] reward= 0.2969369577503375 done= False\n",
      "Step 27\n",
      "Action:  5\n",
      "obs= [19 19 33] reward= 0.5158450640401682 done= False\n",
      "Step 28\n",
      "Action:  5\n",
      "obs= [19 19 34] reward= 0.3109909026708565 done= False\n",
      "Step 29\n",
      "Action:  5\n",
      "obs= [19 19 35] reward= 0.17897133462295198 done= False\n",
      "Step 30\n",
      "Action:  5\n",
      "obs= [19 19 36] reward= 0.1226499675992429 done= False\n",
      "Step 31\n",
      "Action:  5\n",
      "obs= [19 19 37] reward= 0.09278732195078872 done= False\n",
      "Step 32\n",
      "Action:  5\n",
      "obs= [19 19 38] reward= 0.07448021308139116 done= False\n",
      "Step 33\n",
      "Action:  5\n",
      "obs= [19 19 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.5\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [ 5  8 37] reward= 0.0259518128920114 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [ 5  8 38] reward= 0.027887139393514407 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [ 5  8 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.6\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [ 0 17 37] reward= 0.030362433557538294 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [ 0 17 38] reward= 0.03304809563719759 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [ 0 17 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.7\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [18 26  6] reward= 0.022472117501061303 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [18 26  7] reward= 0.02381100092582112 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [18 26  8] reward= 0.025299126402914648 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [18 26  9] reward= 0.02695657983407738 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [18 26 10] reward= 0.028804418588843892 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [18 26 11] reward= 0.030862825464150714 done= False\n",
      "Step 7\n",
      "Action:  5\n",
      "obs= [18 26 12] reward= 0.0331476282295602 done= False\n",
      "Step 8\n",
      "Action:  5\n",
      "obs= [18 26 13] reward= 0.0356645044000579 done= False\n",
      "Step 9\n",
      "Action:  5\n",
      "obs= [18 26 14] reward= 0.03840069814222734 done= False\n",
      "Step 10\n",
      "Action:  5\n",
      "obs= [18 26 15] reward= 0.04131592472384813 done= False\n",
      "Step 11\n",
      "Action:  5\n",
      "obs= [18 26 16] reward= 0.044337841147081246 done= False\n",
      "Step 12\n",
      "Action:  5\n",
      "obs= [18 26 17] reward= 0.047370967099088605 done= False\n",
      "Step 13\n",
      "Action:  5\n",
      "obs= [18 26 18] reward= 0.050324593613916294 done= False\n",
      "Step 14\n",
      "Action:  5\n",
      "obs= [18 26 19] reward= 0.05315081856022594 done= False\n",
      "Step 15\n",
      "Action:  5\n",
      "obs= [18 26 20] reward= 0.055871842801860805 done= False\n",
      "Step 16\n",
      "Action:  5\n",
      "obs= [18 26 21] reward= 0.05858775762949221 done= False\n",
      "Step 17\n",
      "Action:  5\n",
      "obs= [18 26 22] reward= 0.06148852225155032 done= False\n",
      "Step 18\n",
      "Action:  5\n",
      "obs= [18 26 23] reward= 0.06514236071895795 done= False\n",
      "Step 19\n",
      "Action:  5\n",
      "obs= [18 26 24] reward= 0.07026939681329664 done= False\n",
      "Step 20\n",
      "Action:  5\n",
      "obs= [18 26 25] reward= 0.07754111942969866 done= False\n",
      "Step 21\n",
      "Action:  5\n",
      "obs= [18 26 26] reward= 0.08814117778946004 done= False\n",
      "Step 22\n",
      "Action:  5\n",
      "obs= [18 26 27] reward= 0.10446270354142279 done= False\n",
      "Step 23\n",
      "Action:  5\n",
      "obs= [18 26 28] reward= 0.1293392564396578 done= False\n",
      "Step 24\n",
      "Action:  5\n",
      "obs= [18 26 29] reward= 0.1613783296895998 done= False\n",
      "Step 25\n",
      "Action:  5\n",
      "obs= [18 26 30] reward= 0.18412084245212912 done= False\n",
      "Step 26\n",
      "Action:  5\n",
      "obs= [18 26 31] reward= 0.17120812395390844 done= False\n",
      "Step 27\n",
      "Action:  5\n",
      "obs= [18 26 32] reward= 0.1376440436021078 done= False\n",
      "Step 28\n",
      "Action:  5\n",
      "obs= [18 26 33] reward= 0.10797808917442143 done= False\n",
      "Step 29\n",
      "Action:  5\n",
      "obs= [18 26 34] reward= 0.08664210849767445 done= False\n",
      "Step 30\n",
      "Action:  5\n",
      "obs= [18 26 35] reward= 0.07157938155597413 done= False\n",
      "Step 31\n",
      "Action:  5\n",
      "obs= [18 26 36] reward= 0.060669043917789864 done= False\n",
      "Step 32\n",
      "Action:  5\n",
      "obs= [18 26 37] reward= 0.052504652758626064 done= False\n",
      "Step 33\n",
      "Action:  5\n",
      "obs= [18 26 38] reward= 0.04620737791652681 done= False\n",
      "Step 34\n",
      "Action:  5\n",
      "obs= [18 26 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.8\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [11  8 10] reward= 0.013032594036815285 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [11  8 11] reward= 0.01344049336637414 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [11  8 12] reward= 0.013858913238276906 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [11  8 13] reward= 0.014282590487044187 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [11  8 14] reward= 0.014703908801286096 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [11  8 15] reward= 0.015113011311920206 done= False\n",
      "Step 7\n",
      "Action:  5\n",
      "obs= [11  8 16] reward= 0.015498978595227213 done= False\n",
      "Step 8\n",
      "Action:  5\n",
      "obs= [11  8 17] reward= 0.015852405883709272 done= False\n",
      "Step 9\n",
      "Action:  5\n",
      "obs= [11  8 18] reward= 0.01616876285511417 done= False\n",
      "Step 10\n",
      "Action:  5\n",
      "obs= [11  8 19] reward= 0.016450903999065545 done= False\n",
      "Step 11\n",
      "Action:  5\n",
      "obs= [11  8 20] reward= 0.01670949791458866 done= False\n",
      "Step 12\n",
      "Action:  5\n",
      "obs= [11  8 21] reward= 0.01696197590732817 done= False\n",
      "Step 13\n",
      "Action:  5\n",
      "obs= [11  8 22] reward= 0.017231915021397162 done= False\n",
      "Step 14\n",
      "Action:  5\n",
      "obs= [11  8 23] reward= 0.017560227584518168 done= False\n",
      "Step 15\n",
      "Action:  5\n",
      "obs= [11  8 24] reward= 0.017984106642755637 done= False\n",
      "Step 16\n",
      "Action:  5\n",
      "obs= [11  8 25] reward= 0.018523480778418158 done= False\n",
      "Step 17\n",
      "Action:  5\n",
      "obs= [11  8 26] reward= 0.019199854793744656 done= False\n",
      "Step 18\n",
      "Action:  5\n",
      "obs= [11  8 27] reward= 0.020039435186849804 done= False\n",
      "Step 19\n",
      "Action:  5\n",
      "obs= [11  8 28] reward= 0.021048579178019684 done= False\n",
      "Step 20\n",
      "Action:  5\n",
      "obs= [11  8 29] reward= 0.022217368490883932 done= False\n",
      "Step 21\n",
      "Action:  5\n",
      "obs= [11  8 30] reward= 0.023559449886168374 done= False\n",
      "Step 22\n",
      "Action:  5\n",
      "obs= [11  8 31] reward= 0.02509905179526156 done= False\n",
      "Step 23\n",
      "Action:  5\n",
      "obs= [11  8 32] reward= 0.026871717301047066 done= False\n",
      "Step 24\n",
      "Action:  5\n",
      "obs= [11  8 33] reward= 0.028926621318587545 done= False\n",
      "Step 25\n",
      "Action:  5\n",
      "obs= [11  8 34] reward= 0.03133096102001784 done= False\n",
      "Step 26\n",
      "Action:  5\n",
      "obs= [11  8 35] reward= 0.0341772755392976 done= False\n",
      "Step 27\n",
      "Action:  5\n",
      "obs= [11  8 36] reward= 0.0375954031754053 done= False\n",
      "Step 28\n",
      "Action:  5\n",
      "obs= [11  8 37] reward= 0.04177238254579807 done= False\n",
      "Step 29\n",
      "Action:  5\n",
      "obs= [11  8 38] reward= 0.046986822138657444 done= False\n",
      "Step 30\n",
      "Action:  5\n",
      "obs= [11  8 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.9\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [15  3 13] reward= 0.013434030684018692 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [15  3 14] reward= 0.01380604342369375 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [15  3 15] reward= 0.014165886669537428 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [15  3 16] reward= 0.014504104467182892 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [15  3 17] reward= 0.014812739130468081 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [15  3 18] reward= 0.015088235396271514 done= False\n",
      "Step 7\n",
      "Action:  5\n",
      "obs= [15  3 19] reward= 0.015333527216887613 done= False\n",
      "Step 8\n",
      "Action:  5\n",
      "obs= [15  3 20] reward= 0.015558280478196379 done= False\n",
      "Step 9\n",
      "Action:  5\n",
      "obs= [15  3 21] reward= 0.015777875212881206 done= False\n",
      "Step 10\n",
      "Action:  5\n",
      "obs= [15  3 22] reward= 0.016012793438447814 done= False\n",
      "Step 11\n",
      "Action:  5\n",
      "obs= [15  3 23] reward= 0.016297985609767824 done= False\n",
      "Step 12\n",
      "Action:  5\n",
      "obs= [15  3 24] reward= 0.016664732483972065 done= False\n",
      "Step 13\n",
      "Action:  5\n",
      "obs= [15  3 25] reward= 0.017129130328308012 done= False\n",
      "Step 14\n",
      "Action:  5\n",
      "obs= [15  3 26] reward= 0.01770797797271317 done= False\n",
      "Step 15\n",
      "Action:  5\n",
      "obs= [15  3 27] reward= 0.018421137546502625 done= False\n",
      "Step 16\n",
      "Action:  5\n",
      "obs= [15  3 28] reward= 0.019271174191688587 done= False\n",
      "Step 17\n",
      "Action:  5\n",
      "obs= [15  3 29] reward= 0.020247014713792873 done= False\n",
      "Step 18\n",
      "Action:  5\n",
      "obs= [15  3 30] reward= 0.02135649124005314 done= False\n",
      "Step 19\n",
      "Action:  5\n",
      "obs= [15  3 31] reward= 0.02261509883170952 done= False\n",
      "Step 20\n",
      "Action:  5\n",
      "obs= [15  3 32] reward= 0.024045952686046288 done= False\n",
      "Step 21\n",
      "Action:  5\n",
      "obs= [15  3 33] reward= 0.025680752997949707 done= False\n",
      "Step 22\n",
      "Action:  5\n",
      "obs= [15  3 34] reward= 0.02756192991178908 done= False\n",
      "Step 23\n",
      "Action:  5\n",
      "obs= [15  3 35] reward= 0.02974626820658004 done= False\n",
      "Step 24\n",
      "Action:  5\n",
      "obs= [15  3 36] reward= 0.032310666769310596 done= False\n",
      "Step 25\n",
      "Action:  5\n",
      "obs= [15  3 37] reward= 0.035361286103574766 done= False\n",
      "Step 26\n",
      "Action:  5\n",
      "obs= [15  3 38] reward= 0.03904842677220127 done= False\n",
      "Step 27\n",
      "Action:  5\n",
      "obs= [15  3 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n",
      "Test: NO.10\n",
      "Step 1\n",
      "Action:  5\n",
      "obs= [11 23 34] reward= 0.12077407431910783 done= False\n",
      "Step 2\n",
      "Action:  5\n",
      "obs= [11 23 35] reward= 0.16366627717369286 done= False\n",
      "Step 3\n",
      "Action:  5\n",
      "obs= [11 23 36] reward= 0.22123115206216265 done= False\n",
      "Step 4\n",
      "Action:  5\n",
      "obs= [11 23 37] reward= 0.23618869491821398 done= False\n",
      "Step 5\n",
      "Action:  5\n",
      "obs= [11 23 38] reward= 0.18302019337608702 done= False\n",
      "Step 6\n",
      "Action:  5\n",
      "obs= [11 23 38] reward= -1 done= True\n",
      "Goal reached! reward= -1\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "for i in range(10):\n",
    "    print('Test: NO.{}'.format(i+1))\n",
    "    obs = env.reset()\n",
    "    n_steps = 50\n",
    "    for step in range(n_steps):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(\"Step {}\".format(step + 1))\n",
    "        print(\"Action: \", action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "        # env.render()\n",
    "        if done:\n",
    "            # Note that the VecEnv resets automatically\n",
    "            # when a done signal is encountered\n",
    "            print(\"Goal reached!\", \"reward=\", reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if nvidia gpu (cuda) is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "print(torch.cuda.is_available()) # true æ¥çGPUæ¯å¦å¯ç¨\n",
    "\n",
    "print(torch.cuda.device_count()) #GPUæ°éï¼ 1\n",
    "\n",
    "torch.cuda.current_device() #å½åGPUçç´¢å¼ï¼ 0\n",
    "\n",
    "torch.cuda.get_device_name(0) #è¾åºGPUåç§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.8_r_learning_gym",
   "language": "python",
   "name": "r_learning_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
